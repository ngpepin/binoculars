{
  "binoculars_gui_state": true,
  "version": 1,
  "saved_at": "2026-02-14T01:58:13Z",
  "document_path": "/home/npepin/Projects/binoculars/README.md",
  "document_basename": "README.md",
  "text_sha256": "bbcf430d692149c3e1396186c44a31f603049ea26c56e04d695a5d15d098d20c",
  "state": {
    "baseline_text": "# Binoculars\n\n`Binoculars` is a local workflow tool for AI text forensics and humanization. It uses two related `llama.cpp` models - observer and performer - to compute Binoculars-style scores from full logits, then guides you through iterative rewrites of high-risk text spans in the GUI.\n\nIf you need to evaluate or revise long-form text without sending documents to a remote detector, this repository provides a practical solution.\n\n- Data and models remain local by default.\n- The system computes inspectable signals (`logPPL`, `logXPPL`, `B`) rather than opaque labels.\n- Paragraph-level heatmaps show exactly where score pressure originates.\n- For any flagged line or selected block, you can generate three rewrite options and rank them by estimated B impact for quick humanization passes.\n- Full analysis can be re-run only when you need exact checkpoint scores.\n- Optionally, you may use an OpenAI-compatible rewrite backend, with automatic fallback to internal local generation if needed.\n\nReference paper (also in `background/`): https://arxiv.org/abs/2401.12070\n\nAcknowledgment: The Binoculars paper authors - Abhimanyu Hans, Avi Schwarzschild, Valeriia Cherepanova, Hamid Kazemi, Aniruddha Saha, Micah Goldblum, Jonas Geiping, and Tom Goldstein - provided the foundational method for this project.\n\n<p align=\"center\"> <img src=\"media/screenshot.png\" width=\"900\"> <br/> <em>Screenshot of the tkinter GUI</em> </p>\n<p align=\"center\"> <img src=\"media/screenshot-vs.png\" width=\"900\"> <br/> <em>Screenshot of the VS Code extension</em> </p>\n\n## VS Code Extension\n\nThis repository also includes a native VS Code extension in `vscode-extension/` that provides the Binoculars workflow directly inside the editor.\n\nCurrent extension capabilities:\n\n- Analyze current chunk and analyze next chunk.\n- Rewrite current selection/line with ranked options.\n- LOW/HIGH text overlays plus per-line contribution gutter bars.\n- One-click `Clear Priors` plus prior contributor faint backgrounds (major contributors only) after re-analysis.\n- `Toggle Colorization` to hide/show text overlays without losing analysis state.\n- Chunk-aware status bar metrics, including `Prior B` when available.\n- Sidecar persistence in `*.json` next to markdown files for restoring analysis state.\n\nDetailed extension guide:\n\n- `USERGUIDE-VC.md`\n\n## What This Does\n\nGiven input text, `binoculars` computes:\n\n- Observer log-perplexity: `logPPL`\n- Cross log-perplexity: `logXPPL` (observer distribution scored against performer distribution)\n- Binoculars ratio: `B = logPPL / logXPPL`\n\nIt can also generate paragraph-level diagnostics and heatmaps.\n\n## Why Local / Full Logits\n\nThe Binoculars-style cross-entropy term depends on full next-token distributions. In practice, this means:\n\n- `logits_all=True` is required\n- Top-k API logprobs are not sufficient for faithful reconstruction\n- Observer and performer tokenizer alignment must be exact\n\nThe reference paper is included at:\n\n- `background/2401.12070v3.pdf`\n\nAdditional local design notes:\n\n- `initial-scoping.md` (local, gitignored)\n\n## Theory\n\nLet a tokenized document be:\n\n$$ x_1,\\, x_2,\\, \\dots,\\, x_T $$\n\nwith observer model $M_o$ and performer model $M_p$.\n\n**Observer log-perplexity:**\n\n$$ \\log \\mathrm{PPL}_{M_o}(x) = -\\frac{1}{T-1} \\sum_{t=1}^{T-1} \\log p_{M_o}(x_{t+1} \\mid x_{\\leq t}) $$\n\n**Cross log-perplexity:**\n\n$$ \\log \\mathrm{XPPL}_{M_o, M_p}(x) = -\\frac{1}{T-1} \\sum_{t=1}^{T-1} \\sum_{v \\in V} p_{M_o}(v \\mid x_{\\leq t}) \\log p_{M_p}(v \\mid x_{\\leq t}) $$\n\n**Binoculars score:**\n\n$$ B(x) = \\frac{\\log \\mathrm{PPL}_{M_o}(x)}{\\log \\mathrm{XPPL}_{M_o, M_p}(x)} $$\n\nThe current UI/CLI interpretation used by this repository:\n\n- Higher `B` is treated as more human-like\n- Lower paragraph `logPPL` is treated as more AI-like for heatmap colouring\n\nImportant: This is a scoring signal, not proof of authorship.\n\n## Repository Layout\n\n- `binoculars.py`: main CLI and GUI application; `binoculars.sh`: wrapper that activates venv, cleans old instances, and forwards Ctrl-C\n- `binocular.sh`: alias wrapper (`exec binoculars.sh`); `config.binoculars.json`: master profile selector (`default` and `profiles`)\n- `config.binoculars.llm.json`: optional OpenAI-compatible rewrite backend config for GUI rewrite suggestions; `config.llama31.cuda12gb.fast.json`: fast profile (currently `text.max_tokens=4096`)\n- `config.llama31.cuda12gb.long.json`: long profile (currently `text.max_tokens=12288`); `USERGUIDE-GUI.md`: detailed GUI user guide and iterative workflow guidance\n- `vscode-extension/`: VS Code extension source, manifest, bridge, and packaging assets\n- `USERGUIDE-VC.md`: detailed VS Code extension user guide\n- `background/2401.12070v3.pdf`: background paper; `samples/`: sample markdown inputs\n- `tests/test_regression_v1_1_x.py`: regression suite; `tests/fixtures/`: fixture docs used by regression tests\n\n## Requirements\n\n- Linux or macOS shell\n- Python 3.10 or newer\n- `numpy`\n- `llama-cpp-python`\n- Optional: `nltk` (for local WordNet synonym expansion in GUI)\n- GGUF models on local disk\n\nInstall into the repository venv:\n\n```bash\nvenv/bin/pip install numpy llama-cpp-python\n```\n\nOptional WordNet setup for richer synonym suggestions:\n\n```bash\nvenv/bin/pip install nltk\nvenv/bin/python - <<'PY'\nimport nltk\nnltk.download(\"wordnet\", quiet=True)\nnltk.download(\"omw-1.4\", quiet=True)\nPY\n```\n\n## Model Files\n\nConfigs in this repository point to local model paths under `models/`, for example:\n\n- Base observer: Llama 3.1 8B Q5_K_M\n- Instruct performer: Llama 3.1 8B Instruct Q5_K_M\n\nYou may use different models if tokenizer and vocabulary alignment are preserved.\n\n## Configuration\n\n### 1) Master Profile Config\n\n`config.binoculars.json` selects profile by label:\n\n```json\n{\n  \"default\": \"long\",\n  \"profiles\": {\n    \"fast\": \"/abs/path/config.llama31.cuda12gb.fast.json\",\n    \"long\": \"/abs/path/config.llama31.cuda12gb.long.json\"\n  }\n}\n```\n\n`profiles` entries can be either:\n\n- String path (current repository default), or\n- Object form:\n\n```json\n{\n  \"path\": \"/abs/path/config.json\",\n  \"max_tokens\": 8192\n}\n```\n\n`max_tokens` in object form (if present) overrides `text.max_tokens` in the concrete profile.\n\n### 2) Concrete Observer/Performer Profile\n\nEach profile must define:\n\n- `observer`\n- `performer`\n\nOptional blocks:\n\n- `text` (`add_bos`, `special_tokens`, `max_tokens`)\n- `cache` (`dir`, `dtype`, `keep`)\n\nNotes:\n\n- `n_ctx: 0` means auto (`n_ctx = analyzed token count`)\n- `text.max_tokens > 0` truncates input token window\n- `cache.dtype` may be `float16` or `float32`\n\n### 3) Optional Rewrite LLM Config (GUI)\n\nIf `config.binoculars.llm.json` is present and enabled, GUI rewrite suggestions can use an external OpenAI-compatible endpoint. If missing or disabled, internal performer-model generation is used. If present but unreachable or invalid at runtime, GUI rewrites automatically fall back to internal generation.\n\nSupported fields include:\n\n- `llm.enabled`; `llm.endpoint_url`\n- `llm.request_path` (default `/chat/completions`); `llm.model`\n- `llm.api_key` or `llm.api_key_env` (also supports `OPENAI_API_KEY` when enabled); `llm.api_key_header` / `llm.api_key_prefix`\n- `llm.timeout_s`; `llm.max_tokens`\n- `llm.temperature`; `llm.top_p`\n- `llm.context_chars_each_side`; `llm.context_paragraphs_each_side`\n- `llm.context_window_max_chars`; `llm.extra_headers`\n- `llm.extra_body`\n\nExample:\n\n```json\n{\n  \"llm\": {\n    \"enabled\": true,\n    \"endpoint_url\": \"http://localhost:4141/v1\",\n    \"model\": \"gpt-4.1\",\n    \"request_path\": \"/chat/completions\",\n    \"api_key_env\": \"OPENAI_API_KEY\",\n    \"max_tokens\": 220,\n    \"temperature\": 0.78,\n    \"top_p\": 0.95,\n    \"context_chars_each_side\": 1800,\n    \"context_paragraphs_each_side\": 2,\n    \"context_window_max_chars\": 5200\n  }\n}\n```\n\n## Execution Model\n\n`binoculars.py` loads models sequentially:\n\n1. Tokenize with observer and performer in `vocab_only=True`\n2. Hard-fail if tokenization differs\n3. Run observer with full logits\n4. Persist observer logits to memmap\n5. Unload observer, load performer\n6. Compute cross-entropy term from observer distribution versus performer logits\n7. Emit metrics and optional diagnostics or heatmap\n\nThis approach keeps VRAM usage lower than concurrent dual-model loading.\n\n## CLI Usage\n\nShow help:\n\n```bash\n./binoculars.sh --help\n```\n\nBasic usage:\n\n```bash\n./binoculars.sh samples/Athens.md\n```\n\nJSON output:\n\n```bash\n./binoculars.sh --config long samples/Athens.md --json\n```\n\nHeatmap mode:\n\n```bash\n./binoculars.sh --config fast --input samples/Athens.md --heatmap --diagnose-top-k 10\n```\n\nDiagnostics:\n\n```bash\n./binoculars.sh --diagnose-paragraphs --diagnose-top-k 10 samples/Athens.md\n./binoculars.sh --diagnose-paragraphs --diagnose-print-text samples/Athens.md\n```\n\nRun from any directory:\n\n```bash\ncd ~\n/home/npepin/Projects/binoculars/binoculars.sh --config long /tmp/doc.md --json\n```\n\nAlias wrapper:\n\n```bash\n/home/npepin/Projects/binoculars/binocular.sh --config fast /tmp/doc.md\n```\n\n### Input Rules\n\n- Provide input as either:\n  - Positional `INPUT`, or\n  - `--input INPUT`\n- If both are provided, the command errors\n- If multiple positional paths are given, only the first is used (a warning is emitted)\n- If no input is given, stdin (`-`) is used\n\n### CLI Options\n\n- `--master-config FILE`: master profile mapping file; `--config PROFILE`: profile label (`fast`, `long`, etc.)\n- `--input FILE|-`: explicit input; `--output FILE`: write text output\n- `--json`: emit JSON result object; `--diagnose-paragraphs`: rank low-perplexity hotspot paragraphs\n- `--diagnose-top-k N`: hotspot count (also used by heatmap selection); `--diagnose-print-text`: print full hotspot text segments\n- `--heatmap`: emit console and markdown heatmap output; `--gui FILE`: launch interactive GUI editor/analyzer\n\n`--heatmap` cannot be combined with `--json`.\n\n`--gui` is mutually exclusive with:\n\n- `--input`\n- Positional `INPUT`\n- `--output`\n- `--json`\n- `--heatmap`\n\n## Heatmap Mode (`--Heatmap`)\n\nWhen enabled:\n\n- Console output shows:\n  - Red and green highlights (ANSI)\n  - Simple note markers like `[1]`\n  - Line-drawing notes table\n  - Wrapped layout (about 85% terminal width)\n- File output writes markdown to:\n  - `[[HTML_BLOCK_6]]_heatmap.md` in the same directory as the source input\n  - Existing heatmap file is backed up first:\n    - `[[HTML_BLOCK_7]].YYYYMMDD_HHMMSS.bak` (timestamp format may vary by implementation helper)\n\nHeatmap semantics:\n\n- Red sections: lowest paragraph `logPPL`\n- Green sections: highest paragraph `logPPL`\n- Note table columns:\n  - `Index`; `Label`\n  - `% contribution`; `Paragraph`\n  - `logPPL`; `delta_vs_doc`\n  - `delta_if_removed`; `Transitions`\n  - `Chars`; `Tokens`\n\n## GUI Mode (`--Gui [[HTML_BLOCK_8]]`)\n\nLaunches an editor/analyzer with:\n\n- Window/app name: `Binoculars`\n- Left pane: editable source text\n- Left gutter:\n  - Logical line numbers\n  - Red and green contribution bars per line\n- Right pane: live markdown preview\n- Right-pane footer: real-time synonym panel for clicked words\n- Controls:\n  - `Open`\n  - `Analyze`\n  - `Analyze Next` (shown after first chunk when unscored text remains)\n  - `Save`\n  - `Undo` (one level)\n  - `Clear Priors`\n  - `Quit`\n- Status bar:\n  - `Binocular score B (high is more human-like): ...`\n  - Shows active chunk range and chunk-relative metrics\n\nFor a detailed workflow-oriented guide, see:\n\n- `USERGUIDE-GUI.md`\n\n### GUI Behaviour\n\n- `Analyze`:\n  - On first run, analyzes from document start (`char 0`) up to the token-limited chunk boundary\n  - On later runs, analyzes from the active chunk start (not from the cursor line itself)\n  - Replaces any overlapping prior chunk descriptor with the new analysis result\n  - Can move the effective chunk end boundary forward or backward after edits\n  - Preserves cursor and top-view position\n  - Updates red and green foreground highlights\n  - Updates hover tooltips (`% contribution`, `logPPL`, `delta_if_removed`, `delta_vs_doc`, ranges)\n  - Updates status metrics for the active chunk\n  - Keeps `Performing analysis on current text...` visible until analysis finishes\n  - Edits since last analysis show in yellow\n\n- `Analyze Next`:\n  - Appears after first successful analysis if unscored text remains\n  - Starts at the contiguous covered tail (`analysis_covered_until`)\n  - Extends coverage into the next token-limited chunk\n  - Remains available until contiguous coverage reaches end-of-document\n\n- Active chunk selection priority for status and `Analyze`:\n  - Selection overlap with analyzed chunks (largest overlap wins)\n  - Else cursor line if visible and inside an analyzed chunk\n  - Else majority overlap with currently visible editor window\n  - Else nearest analyzed chunk by distance\n\n- Chunk boundaries are operational, not immutable:\n  - Chunks are stored as descriptors with local metrics.\n  - Coverage intervals are merged for rendering/unscored display.\n  - Overlapping chunk descriptors are replaced on re-analysis.\n  - Result: boundaries can shift after edits and a second `Analyze`.\n\n- Example:\n  - First pass analyzes lines `1-999` as chunk 1.\n  - Cursor is at line `999` and user presses `Analyze`.\n  - Binoculars re-analyzes starting at chunk-1 start (line 1), not line 999.\n  - If edits changed token density, chunk 1 may now end at line `972` or `1031`.\n  - Status then reports metrics for that updated chunk range.\n\n- On advancing to the next `Analyze`, previous highlights or edits are reduced to faint backgrounds, indicating prior states.\n- Executing `Clear Priors` clears only these faint backgrounds, leaving other markings intact.\n- The `Save` command initiates a write operation:\n- `[[HTML_BLOCK_9]]_edited_YYYYMMDDHHMM.md`\n- Output is directed to the same directory as the source file.\n- During the write process, a modal `Saving...` popup displays the destination filename.\n\nAlways-on English spell checking is active:\n- Misspelled words are underlined in red.\n\nSynonym suggestions are available:\n- Left-click any word in the left pane to request synonyms.\n- Lookup actions are debounced to prevent triggering during drag-selection.\n- The synonym panel displays up to nine options in three columns, each with a `1..9` button.\n- Selecting a synonym replaces the chosen word and marks the change as an edit (yellow).\n- The lookup sequence is: local fallback list, then WordNet (if installed), then Datamuse API as a fallback.\n\nRewrite suggestions can be accessed as follows:\n- Right-click a red (`LOW`) paragraph segment to open three rewrite options. Alternatively, highlight a block (multi-line selection allowed) and right-click to request block rewrites.\n- Highlighted-block rewrites round the selection to full lines. If unscored lines are included, they are preserved unchanged while scored/analyzed lines are rewritten.\n- The popup displays the approximate B impact for each option (exact B requires `Analyze`). Options are sorted by expected B increase, with more human-like choices first.\n- Select an option using the mouse or keyboard (`1`/`2`/`3`), or `Quit`. The selected rewrite is inserted as an edit (yellow), and prior backgrounds are preserved according to previous line status.\n- The B score is not automatically recalculated; the status marks it as stale until the next `Analyze`.\n\nDelete and undo operations are tracked:\n- Deleting a selected block with `Delete` or `Backspace` is recorded as a single undoable action.\n- `Undo` supports one level of undo for selected-block delete, synonym replacement, red-segment rewrite, and block rewrite.\n- Successful undo status is shown briefly, after which metrics return.\n\nPreview selection mirroring is supported:\n- Selecting a block in the left pane causes the right preview to mirror the same line range.\n- Selected preview lines display LOW, HIGH, or neutral background styles.\n\nStatus-bar transient messages:\n- Non-analysis events (such as Save, Clear Priors, Delete, or Undo) temporarily replace metrics.\n- Most transient messages restore metrics after approximately eight seconds.\n- A successful `Undo applied...` restores metrics quickly, in about 1.8 seconds.\n\n### Preview Sync and Debug Controls\n\nEnvironment variables:\n- `BINOCULARS_GUI_DEBUG=1`: Starts with the debug overlay enabled; can be toggled in-app with `F9`.\n- `BINOCULARS_PREVIEW_VIEW_OFFSET_LINES=-3`: Adjusts vertical view calibration for the right pane, affecting only the preview viewport position (not line mapping).\n\n## Wrapper Behaviour (`Binoculars.Sh`)\n\n`binoculars.sh` performs the following:\n- Activates the repository virtual environment.\n- Runs `binoculars.py`.\n- Deactivates the virtual environment on exit.\n- Forwards Ctrl-C to the child process.\n- Terminates prior running instances by default to free GPU or VRAM resources.\n\nTo disable automatic termination if necessary:\n\n```bash\nBINOCULARS_DISABLE_AUTO_KILL=1 ./binoculars.sh ...\n```\n\n## Output Contract (JSON)\n\nTop-level keys include:\n- `input`\n- `observer`\n- `performer`\n- `cross`\n- `binoculars`\n- `cache`\n\nOptional:\n- `diagnostics.low_perplexity_spans` (when `--diagnose-paragraphs` is enabled)\n\n## Performance and Tuning Notes\n\n- Main memory usage is driven by full logits (`tokens x vocab`).\n- Long contexts are resource-intensive, even if VRAM appears available.\n- `text.max_tokens` is the primary limit for runtime and memory safety.\n- `n_ctx: 0` is typically optimal (auto-sizes to analyzed tokens).\n- Observer and performer components are loaded sequentially.\n\nCurrent shipped profile token limits:\n- `fast`: 4096\n- `long`: 12288\n\nAdjust these values in profile JSON files as needed for your hardware.\n\n## Troubleshooting\n\nTokenizer mismatch error:\n- Use a model pair from the same family (base and instruct sibling).\n- Ensure both configurations reference compatible tokenizer and vocabulary models.\n\nMissing file errors:\n- Verify `config.binoculars.json` profile paths.\n- Check model paths in the configuration JSON files.\n\nGUI unavailable:\n- Confirm that Tkinter is installed in your Python environment.\n\nUnexpected GPU memory contention:\n- Close other LLM processes, or rely on the wrapper's auto-kill feature.\n- Reduce `text.max_tokens`.\n- Lower `n_batch` if necessary.\n\nllama.cpp context warnings:\n- Low-signal llama.cpp runtime logs (`INFO`, `WARN`, `DEBUG`, including `llama_context` initialization messages) are suppressed by default for cleaner output.\n- To disable log suppression for debugging, set `BINOCULARS_SUPPRESS_LLAMA_CONTEXT_WARNINGS=0`.\n\n## Tests\n\nTo run the regression suite:\n\n```bash\n./venv/bin/python -m unittest -v tests/test_regression_v1_1_x.py\n```\n\n## License\n\nPolyForm Noncommercial License 1.0.0. This project is licensed under the **PolyForm Noncommercial License 1.0.0** (see `LICENSE.md`).\n\nImportant scope clarification:\n- The licence applies to the code, configuration, scripts, and documentation in this repository.\n- It does not claim ownership of, or restrict use of, the Binoculars approach described in the associated paper.\n\nKey points:\n- Noncommercial use only: use, modification, and redistribution are permitted for noncommercial purposes.\n\n- Commercial use requires explicit authorization. Any paid product or service that incorporates this code must have the author's permission.\n\n- Attribution is mandatory. Redistribution or use of substantial portions of this project must include clear credit and preserve the licence and notice requirements described in `LICENSE.md`.\n\nFor commercial applications, contact the author to discuss participation or licensing.\n\n## Limitations\n\n- No calibrated classifier thresholds are implemented yet.\n- There is no claim of definitive authorship attribution.\n- Markdown is processed as plain text; semantic markdown parsing is not performed.\n- Long documents may need to be truncated due to full-logit computational cost.\n\n## Chunk-Aware Large-File Analysis (Current Behaviour)\n\nFor files that exceed one-pass token limits, GUI analysis is chunk-aware.\n\n- First `Analyze` starts at document start and scores the first token-limited chunk.\n- `Analyze Next` progresses from contiguous covered tail into the next chunk.\n- `Analyze` on later runs targets the active chunk start, not the cursor line.\n- Active chunk metrics drive the status bar and rewrite-approximation baselines.\n- Unscored regions are rendered as complement intervals over full document length.\n- Chunk boundaries may shift after edits because overlapping chunk descriptors are replaced by the newest analysis.\n\n### Chunking FAQ\n\nQ: If my cursor is near the end of chunk 1 (for example line 999) and I press `Analyze`, what gets analyzed?\n\nA: Binoculars analyzes from the active chunk start, not from the cursor line. In that case it re-analyzes from the start of chunk 1 (often line 1) forward to the current token-limited boundary.\n\nQ: Will chunk boundaries stay fixed forever once discovered?\n\nA: No. Chunk metrics are always chunk-local, but chunk boundaries are operational. If text edits change token density, a later re-analysis can move the chunk end earlier or later, and overlapping prior chunk descriptors are replaced by the new result.\n\n## Safety / Responsible Use\n\nTreat outputs as probabilistic signals within a broader review process. Do not use this tool as the sole basis for punitive or high-stakes actions.\n\n## Appendix: GPTZero vs Binoculars (Public Information)\n\nThis appendix summarizes public details about GPTZero and compares them to the Binoculars method implemented here.\n\n### 1) What Is Known About Gptzero\n\nPublic GPTZero materials describe a layered detection system.\n\n- The initial release (January 2023) focused on `perplexity` and `burstiness` as core indicators.\n- Current documentation describes a probabilistic, sentence-level and document-level deep-learning detector that does not rely solely on perplexity or burstiness.\n- GPTZero states its production system combines multiple components and outputs trinary sentence labels (`human`, `mixed`, `AI`) with confidence and uncertainty handling.\n\nDetails not publicly disclosed:\n\n- Exact model architectures.\n- Training data composition.\n- Post-processing and thresholding methods.\n- Adversarial hardening techniques.\n\nTherefore, GPTZero is partially documented, but the full internal mechanisms remain proprietary.\n\n### 2) How Binoculars Differs\n\nBinoculars is more explicit in its mechanism.\n\n- It is defined by transparent equations over two related language models:\n  - observer `logPPL`\n  - observer-vs-performer `logXPPL`\n  - ratio `B = logPPL / logXPPL`\n- It uses a zero-shot detection approach, requiring no target-model-specific training for the detector.\n- The paper reports strong low-FPR performance, detecting over 90% of generated samples at 0.01% FPR in tested scenarios.\n- Head-to-head comparisons in the paper show Binoculars outperforming GPTZero in their 2023 evaluation setup.\n\n### 3) Why Binoculars Can Compete with Gptzero\n\nAvailable evidence indicates Binoculars can operate in the same competitive tier, with some caveats.\n\n- Published results show strong discrimination at very low false-positive rates, a key deployment factor.\n- The mechanism is model-agnostic in the zero-shot sense and can generalize to unseen generators when assumptions hold.\n- The approach is inspectable and reproducible (equations and open implementation), aiding calibration and operational trust.\n\nHowever, caution is warranted.\n\n- “As robust as GPTZero” is context-dependent and should be validated on your domain, document lengths, and attack or perturbation conditions.\n- The Binoculars paper notes important limits, such as degraded recall in some low-resource language settings and no guarantee against motivated adversarial evasion.\n- Independent benchmarks indicate that many detectors degrade under perturbation, so robustness claims should be treated as empirical and subject to ongoing review.\n\nIn practice:\n\n- Binoculars is credibly “same-league” with commercial detectors in several reported scenarios, especially when low-FPR behaviour is required.\n- You should run periodic, domain-specific benchmark checks (including perturbed or paraphrased text) before making strong operational claims.\n\n### 4) Sources\n\n- Binoculars paper (arXiv): https://arxiv.org/abs/2401.12070\n- Binoculars paper in repo: `background/2401.12070v3.pdf`\n- GPTZero technology page: https://gptzero.me/technology\n- GPTZero FAQ (method overview): https://gptzero.me/faqs/how-does-ai-detection-work\n- GPTZero original launch note (perplexity/burstiness framing): https://gptzero.me/news/first-release-of-gptzero-for-educators-january-3-2023\n- RAID benchmark (robustness context): https://arxiv.org/abs/2406.07958\n",
    "prev_text": "# Binoculars\n\n`Binoculars` is a local workflow tool for AI text forensics and humanization. It uses two related `llama.cpp` models - observer and performer - to compute Binoculars-style scores from full logits, then guides you through iterative rewrites of high-risk text spans in the GUI.\n\nIf you need to evaluate or revise long-form text without sending documents to a remote detector, this repository provides a practical solution.\n\n- Data and models remain local by default.\n- The system computes inspectable signals (`logPPL`, `logXPPL`, `B`) rather than opaque labels.\n- Paragraph-level heatmaps show exactly where score pressure originates.\n- For any flagged line or selected block, you can generate three rewrite options and rank them by estimated B impact for quick humanization passes.\n- Full analysis can be re-run only when you need exact checkpoint scores.\n- Optionally, you may use an OpenAI-compatible rewrite backend, with automatic fallback to internal local generation if needed.\n\nReference paper (also in `background/`): https://arxiv.org/abs/2401.12070\n\nAcknowledgment: The Binoculars paper authors - Abhimanyu Hans, Avi Schwarzschild, Valeriia Cherepanova, Hamid Kazemi, Aniruddha Saha, Micah Goldblum, Jonas Geiping, and Tom Goldstein - provided the foundational method for this project.\n\n<p align=\"center\"> <img src=\"media/screenshot.png\" width=\"900\"> <br/> <em>Screenshot of the tkinter GUI</em> </p>\n<p align=\"center\"> <img src=\"media/screenshot-vs.png\" width=\"900\"> <br/> <em>Screenshot of the VS Code extension</em> </p>\n\n## VS Code Extension\n\nThis repository also includes a native VS Code extension in `vscode-extension/` that provides the Binoculars workflow directly inside the editor.\n\nCurrent extension capabilities:\n\n- Analyze current chunk and analyze next chunk.\n- Rewrite current selection/line with ranked options.\n- LOW/HIGH text overlays plus per-line contribution gutter bars.\n- One-click `Clear Priors` plus prior contributor faint backgrounds (major contributors only) after re-analysis.\n- `Toggle Colorization` to hide/show text overlays without losing analysis state.\n- Chunk-aware status bar metrics, including `Prior B` when available.\n- Sidecar persistence in `*.json` next to markdown files for restoring analysis state.\n\nDetailed extension guide:\n\n- `USERGUIDE-VC.md`\n\n## What This Does\n\nGiven input text, `binoculars` computes:\n\n- Observer log-perplexity: `logPPL`\n- Cross log-perplexity: `logXPPL` (observer distribution scored against performer distribution)\n- Binoculars ratio: `B = logPPL / logXPPL`\n\nIt can also generate paragraph-level diagnostics and heatmaps.\n\n## Why Local / Full Logits\n\nThe Binoculars-style cross-entropy term depends on full next-token distributions. In practice, this means:\n\n- `logits_all=True` is required\n- Top-k API logprobs are not sufficient for faithful reconstruction\n- Observer and performer tokenizer alignment must be exact\n\nThe reference paper is included at:\n\n- `background/2401.12070v3.pdf`\n\nAdditional local design notes:\n\n- `initial-scoping.md` (local, gitignored)\n\n## Theory\n\nLet a tokenized document be:\n\n$$ x_1,\\, x_2,\\, \\dots,\\, x_T $$\n\nwith observer model $M_o$ and performer model $M_p$.\n\n**Observer log-perplexity:**\n\n$$ \\log \\mathrm{PPL}_{M_o}(x) = -\\frac{1}{T-1} \\sum_{t=1}^{T-1} \\log p_{M_o}(x_{t+1} \\mid x_{\\leq t}) $$\n\n**Cross log-perplexity:**\n\n$$ \\log \\mathrm{XPPL}_{M_o, M_p}(x) = -\\frac{1}{T-1} \\sum_{t=1}^{T-1} \\sum_{v \\in V} p_{M_o}(v \\mid x_{\\leq t}) \\log p_{M_p}(v \\mid x_{\\leq t}) $$\n\n**Binoculars score:**\n\n$$ B(x) = \\frac{\\log \\mathrm{PPL}_{M_o}(x)}{\\log \\mathrm{XPPL}_{M_o, M_p}(x)} $$\n\nThe current UI/CLI interpretation used by this repository:\n\n- Higher `B` is treated as more human-like\n- Lower paragraph `logPPL` is treated as more AI-like for heatmap colouring\n\nImportant: This is a scoring signal, not proof of authorship.\n\n## Repository Layout\n\n- `binoculars.py`: main CLI and GUI application; `binoculars.sh`: wrapper that activates venv, cleans old instances, and forwards Ctrl-C\n- `binocular.sh`: alias wrapper (`exec binoculars.sh`); `config.binoculars.json`: master profile selector (`default` and `profiles`)\n- `config.binoculars.llm.json`: optional OpenAI-compatible rewrite backend config for GUI rewrite suggestions; `config.llama31.cuda12gb.fast.json`: fast profile (currently `text.max_tokens=4096`)\n- `config.llama31.cuda12gb.long.json`: long profile (currently `text.max_tokens=12288`); `USERGUIDE-GUI.md`: detailed GUI user guide and iterative workflow guidance\n- `vscode-extension/`: VS Code extension source, manifest, bridge, and packaging assets\n- `USERGUIDE-VC.md`: detailed VS Code extension user guide\n- `background/2401.12070v3.pdf`: background paper; `samples/`: sample markdown inputs\n- `tests/test_regression_v1_1_x.py`: regression suite; `tests/fixtures/`: fixture docs used by regression tests\n\n## Requirements\n\n- Linux or macOS shell\n- Python 3.10 or newer\n- `numpy`\n- `llama-cpp-python`\n- Optional: `nltk` (for local WordNet synonym expansion in GUI)\n- GGUF models on local disk\n\nInstall into the repository venv:\n\n```bash\nvenv/bin/pip install numpy llama-cpp-python\n```\n\nOptional WordNet setup for richer synonym suggestions:\n\n```bash\nvenv/bin/pip install nltk\nvenv/bin/python - <<'PY'\nimport nltk\nnltk.download(\"wordnet\", quiet=True)\nnltk.download(\"omw-1.4\", quiet=True)\nPY\n```\n\n## Model Files\n\nConfigs in this repository point to local model paths under `models/`, for example:\n\n- Base observer: Llama 3.1 8B Q5_K_M\n- Instruct performer: Llama 3.1 8B Instruct Q5_K_M\n\nYou may use different models if tokenizer and vocabulary alignment are preserved.\n\n## Configuration\n\n### 1) Master Profile Config\n\n`config.binoculars.json` selects profile by label:\n\n```json\n{\n  \"default\": \"long\",\n  \"profiles\": {\n    \"fast\": \"/abs/path/config.llama31.cuda12gb.fast.json\",\n    \"long\": \"/abs/path/config.llama31.cuda12gb.long.json\"\n  }\n}\n```\n\n`profiles` entries can be either:\n\n- String path (current repository default), or\n- Object form:\n\n```json\n{\n  \"path\": \"/abs/path/config.json\",\n  \"max_tokens\": 8192\n}\n```\n\n`max_tokens` in object form (if present) overrides `text.max_tokens` in the concrete profile.\n\n### 2) Concrete Observer/Performer Profile\n\nEach profile must define:\n\n- `observer`\n- `performer`\n\nOptional blocks:\n\n- `text` (`add_bos`, `special_tokens`, `max_tokens`)\n- `cache` (`dir`, `dtype`, `keep`)\n\nNotes:\n\n- `n_ctx: 0` means auto (`n_ctx = analyzed token count`)\n- `text.max_tokens > 0` truncates input token window\n- `cache.dtype` may be `float16` or `float32`\n\n### 3) Optional Rewrite LLM Config (GUI)\n\nIf `config.binoculars.llm.json` is present and enabled, GUI rewrite suggestions can use an external OpenAI-compatible endpoint. If missing or disabled, internal performer-model generation is used. If present but unreachable or invalid at runtime, GUI rewrites automatically fall back to internal generation.\n\nSupported fields include:\n\n- `llm.enabled`; `llm.endpoint_url`\n- `llm.request_path` (default `/chat/completions`); `llm.model`\n- `llm.api_key` or `llm.api_key_env` (also supports `OPENAI_API_KEY` when enabled); `llm.api_key_header` / `llm.api_key_prefix`\n- `llm.timeout_s`; `llm.max_tokens`\n- `llm.temperature`; `llm.top_p`\n- `llm.context_chars_each_side`; `llm.context_paragraphs_each_side`\n- `llm.context_window_max_chars`; `llm.extra_headers`\n- `llm.extra_body`\n\nExample:\n\n```json\n{\n  \"llm\": {\n    \"enabled\": true,\n    \"endpoint_url\": \"http://localhost:4141/v1\",\n    \"model\": \"gpt-4.1\",\n    \"request_path\": \"/chat/completions\",\n    \"api_key_env\": \"OPENAI_API_KEY\",\n    \"max_tokens\": 220,\n    \"temperature\": 0.78,\n    \"top_p\": 0.95,\n    \"context_chars_each_side\": 1800,\n    \"context_paragraphs_each_side\": 2,\n    \"context_window_max_chars\": 5200\n  }\n}\n```\n\n## Execution Model\n\n`binoculars.py` loads models sequentially:\n\n1. Tokenize with observer and performer in `vocab_only=True`\n2. Hard-fail if tokenization differs\n3. Run observer with full logits\n4. Persist observer logits to memmap\n5. Unload observer, load performer\n6. Compute cross-entropy term from observer distribution versus performer logits\n7. Emit metrics and optional diagnostics or heatmap\n\nThis approach keeps VRAM usage lower than concurrent dual-model loading.\n\n## CLI Usage\n\nShow help:\n\n```bash\n./binoculars.sh --help\n```\n\nBasic usage:\n\n```bash\n./binoculars.sh samples/Athens.md\n```\n\nJSON output:\n\n```bash\n./binoculars.sh --config long samples/Athens.md --json\n```\n\nHeatmap mode:\n\n```bash\n./binoculars.sh --config fast --input samples/Athens.md --heatmap --diagnose-top-k 10\n```\n\nDiagnostics:\n\n```bash\n./binoculars.sh --diagnose-paragraphs --diagnose-top-k 10 samples/Athens.md\n./binoculars.sh --diagnose-paragraphs --diagnose-print-text samples/Athens.md\n```\n\nRun from any directory:\n\n```bash\ncd ~\n/home/npepin/Projects/binoculars/binoculars.sh --config long /tmp/doc.md --json\n```\n\nAlias wrapper:\n\n```bash\n/home/npepin/Projects/binoculars/binocular.sh --config fast /tmp/doc.md\n```\n\n### Input Rules\n\n- Provide input as either:\n  - Positional `INPUT`, or\n  - `--input INPUT`\n- If both are provided, the command errors\n- If multiple positional paths are given, only the first is used (a warning is emitted)\n- If no input is given, stdin (`-`) is used\n\n### CLI Options\n\n- `--master-config FILE`: master profile mapping file; `--config PROFILE`: profile label (`fast`, `long`, etc.)\n- `--input FILE|-`: explicit input; `--output FILE`: write text output\n- `--json`: emit JSON result object; `--diagnose-paragraphs`: rank low-perplexity hotspot paragraphs\n- `--diagnose-top-k N`: hotspot count (also used by heatmap selection); `--diagnose-print-text`: print full hotspot text segments\n- `--heatmap`: emit console and markdown heatmap output; `--gui FILE`: launch interactive GUI editor/analyzer\n\n`--heatmap` cannot be combined with `--json`.\n\n`--gui` is mutually exclusive with:\n\n- `--input`\n- Positional `INPUT`\n- `--output`\n- `--json`\n- `--heatmap`\n\n## Heatmap Mode (`--Heatmap`)\n\nWhen enabled:\n\n- Console output shows:\n  - Red and green highlights (ANSI)\n  - Simple note markers like `[1]`\n  - Line-drawing notes table\n  - Wrapped layout (about 85% terminal width)\n- File output writes markdown to:\n  - `[[HTML_BLOCK_6]]_heatmap.md` in the same directory as the source input\n  - Existing heatmap file is backed up first:\n    - `[[HTML_BLOCK_7]].YYYYMMDD_HHMMSS.bak` (timestamp format may vary by implementation helper)\n\nHeatmap semantics:\n\n- Red sections: lowest paragraph `logPPL`\n- Green sections: highest paragraph `logPPL`\n- Note table columns:\n  - `Index`; `Label`\n  - `% contribution`; `Paragraph`\n  - `logPPL`; `delta_vs_doc`\n  - `delta_if_removed`; `Transitions`\n  - `Chars`; `Tokens`\n\n## GUI Mode (`--Gui [[HTML_BLOCK_8]]`)\n\nLaunches an editor/analyzer with:\n\n- Window/app name: `Binoculars`\n- Left pane: editable source text\n- Left gutter:\n  - Logical line numbers\n  - Red and green contribution bars per line\n- Right pane: live markdown preview\n- Right-pane footer: real-time synonym panel for clicked words\n- Controls:\n  - `Open`\n  - `Analyze`\n  - `Analyze Next` (shown after first chunk when unscored text remains)\n  - `Save`\n  - `Undo` (one level)\n  - `Clear Priors`\n  - `Quit`\n- Status bar:\n  - `Binocular score B (high is more human-like): ...`\n  - Shows active chunk range and chunk-relative metrics\n\nFor a detailed workflow-oriented guide, see:\n\n- `USERGUIDE-GUI.md`\n\n### GUI Behaviour\n\n- `Analyze`:\n  - On first run, analyzes from document start (`char 0`) up to the token-limited chunk boundary\n  - On later runs, analyzes from the active chunk start (not from the cursor line itself)\n  - Replaces any overlapping prior chunk descriptor with the new analysis result\n  - Can move the effective chunk end boundary forward or backward after edits\n  - Preserves cursor and top-view position\n  - Updates red and green foreground highlights\n  - Updates hover tooltips (`% contribution`, `logPPL`, `delta_if_removed`, `delta_vs_doc`, ranges)\n  - Updates status metrics for the active chunk\n  - Keeps `Performing analysis on current text...` visible until analysis finishes\n  - Edits since last analysis show in yellow\n\n- `Analyze Next`:\n  - Appears after first successful analysis if unscored text remains\n  - Starts at the contiguous covered tail (`analysis_covered_until`)\n  - Extends coverage into the next token-limited chunk\n  - Remains available until contiguous coverage reaches end-of-document\n\n- Active chunk selection priority for status and `Analyze`:\n  - Selection overlap with analyzed chunks (largest overlap wins)\n  - Else cursor line if visible and inside an analyzed chunk\n  - Else majority overlap with currently visible editor window\n  - Else nearest analyzed chunk by distance\n\n- Chunk boundaries are operational, not immutable:\n  - Chunks are stored as descriptors with local metrics.\n  - Coverage intervals are merged for rendering/unscored display.\n  - Overlapping chunk descriptors are replaced on re-analysis.\n  - Result: boundaries can shift after edits and a second `Analyze`.\n\n- Example:\n  - First pass analyzes lines `1-999` as chunk 1.\n  - Cursor is at line `999` and user presses `Analyze`.\n  - Binoculars re-analyzes starting at chunk-1 start (line 1), not line 999.\n  - If edits changed token density, chunk 1 may now end at line `972` or `1031`.\n  - Status then reports metrics for that updated chunk range.\n\n- On advancing to the next `Analyze`, previous highlights or edits are reduced to faint backgrounds, indicating prior states.\n- Executing `Clear Priors` clears only these faint backgrounds, leaving other markings intact.\n- The `Save` command initiates a write operation:\n- `[[HTML_BLOCK_9]]_edited_YYYYMMDDHHMM.md`\n- Output is directed to the same directory as the source file.\n- During the write process, a modal `Saving...` popup displays the destination filename.\n\nAlways-on English spell checking is active:\n- Misspelled words are underlined in red.\n\nSynonym suggestions are available:\n- Left-click any word in the left pane to request synonyms.\n- Lookup actions are debounced to prevent triggering during drag-selection.\n- The synonym panel displays up to nine options in three columns, each with a `1..9` button.\n- Selecting a synonym replaces the chosen word and marks the change as an edit (yellow).\n- The lookup sequence is: local fallback list, then WordNet (if installed), then Datamuse API as a fallback.\n\nRewrite suggestions can be accessed as follows:\n- Right-click a red (`LOW`) paragraph segment to open three rewrite options. Alternatively, highlight a block (multi-line selection allowed) and right-click to request block rewrites.\n- Highlighted-block rewrites round the selection to full lines. If unscored lines are included, they are preserved unchanged while scored/analyzed lines are rewritten.\n- The popup displays the approximate B impact for each option (exact B requires `Analyze`). Options are sorted by expected B increase, with more human-like choices first.\n- Select an option using the mouse or keyboard (`1`/`2`/`3`), or `Quit`. The selected rewrite is inserted as an edit (yellow), and prior backgrounds are preserved according to previous line status.\n- The B score is not automatically recalculated; the status marks it as stale until the next `Analyze`.\n\nDelete and undo operations are tracked:\n- Deleting a selected block with `Delete` or `Backspace` is recorded as a single undoable action.\n- `Undo` supports one level of undo for selected-block delete, synonym replacement, red-segment rewrite, and block rewrite.\n- Successful undo status is shown briefly, after which metrics return.\n\nPreview selection mirroring is supported:\n- Selecting a block in the left pane causes the right preview to mirror the same line range.\n- Selected preview lines display LOW, HIGH, or neutral background styles.\n\nStatus-bar transient messages:\n- Non-analysis events (such as Save, Clear Priors, Delete, or Undo) temporarily replace metrics.\n- Most transient messages restore metrics after approximately eight seconds.\n- A successful `Undo applied...` restores metrics quickly, in about 1.8 seconds.\n\n### Preview Sync and Debug Controls\n\nEnvironment variables:\n- `BINOCULARS_GUI_DEBUG=1`: Starts with the debug overlay enabled; can be toggled in-app with `F9`.\n- `BINOCULARS_PREVIEW_VIEW_OFFSET_LINES=-3`: Adjusts vertical view calibration for the right pane, affecting only the preview viewport position (not line mapping).\n\n## Wrapper Behaviour (`Binoculars.Sh`)\n\n`binoculars.sh` performs the following:\n- Activates the repository virtual environment.\n- Runs `binoculars.py`.\n- Deactivates the virtual environment on exit.\n- Forwards Ctrl-C to the child process.\n- Terminates prior running instances by default to free GPU or VRAM resources.\n\nTo disable automatic termination if necessary:\n\n```bash\nBINOCULARS_DISABLE_AUTO_KILL=1 ./binoculars.sh ...\n```\n\n## Output Contract (JSON)\n\nTop-level keys include:\n- `input`\n- `observer`\n- `performer`\n- `cross`\n- `binoculars`\n- `cache`\n\nOptional:\n- `diagnostics.low_perplexity_spans` (when `--diagnose-paragraphs` is enabled)\n\n## Performance and Tuning Notes\n\n- Main memory usage is driven by full logits (`tokens x vocab`).\n- Long contexts are resource-intensive, even if VRAM appears available.\n- `text.max_tokens` is the primary limit for runtime and memory safety.\n- `n_ctx: 0` is typically optimal (auto-sizes to analyzed tokens).\n- Observer and performer components are loaded sequentially.\n\nCurrent shipped profile token limits:\n- `fast`: 4096\n- `long`: 12288\n\nAdjust these values in profile JSON files as needed for your hardware.\n\n## Troubleshooting\n\nTokenizer mismatch error:\n- Use a model pair from the same family (base and instruct sibling).\n- Ensure both configurations reference compatible tokenizer and vocabulary models.\n\nMissing file errors:\n- Verify `config.binoculars.json` profile paths.\n- Check model paths in the configuration JSON files.\n\nGUI unavailable:\n- Confirm that Tkinter is installed in your Python environment.\n\nUnexpected GPU memory contention:\n- Close other LLM processes, or rely on the wrapper's auto-kill feature.\n- Reduce `text.max_tokens`.\n- Lower `n_batch` if necessary.\n\nllama.cpp context warnings:\n- Low-signal llama.cpp runtime logs (`INFO`, `WARN`, `DEBUG`, including `llama_context` initialization messages) are suppressed by default for cleaner output.\n- To disable log suppression for debugging, set `BINOCULARS_SUPPRESS_LLAMA_CONTEXT_WARNINGS=0`.\n\n## Tests\n\nTo run the regression suite:\n\n```bash\n./venv/bin/python -m unittest -v tests/test_regression_v1_1_x.py\n```\n\n## License\n\nPolyForm Noncommercial License 1.0.0. This project is licensed under the **PolyForm Noncommercial License 1.0.0** (see `LICENSE.md`).\n\nImportant scope clarification:\n- The licence applies to the code, configuration, scripts, and documentation in this repository.\n- It does not claim ownership of, or restrict use of, the Binoculars approach described in the associated paper.\n\nKey points:\n- Noncommercial use only: use, modification, and redistribution are permitted for noncommercial purposes.\n\n- Commercial use requires explicit authorization. Any paid product or service that incorporates this code must have the author's permission.\n\n- Attribution is mandatory. Redistribution or use of substantial portions of this project must include clear credit and preserve the licence and notice requirements described in `LICENSE.md`.\n\nFor commercial applications, contact the author to discuss participation or licensing.\n\n## Limitations\n\n- No calibrated classifier thresholds are implemented yet.\n- There is no claim of definitive authorship attribution.\n- Markdown is processed as plain text; semantic markdown parsing is not performed.\n- Long documents may need to be truncated due to full-logit computational cost.\n\n## Chunk-Aware Large-File Analysis (Current Behaviour)\n\nFor files that exceed one-pass token limits, GUI analysis is chunk-aware.\n\n- First `Analyze` starts at document start and scores the first token-limited chunk.\n- `Analyze Next` progresses from contiguous covered tail into the next chunk.\n- `Analyze` on later runs targets the active chunk start, not the cursor line.\n- Active chunk metrics drive the status bar and rewrite-approximation baselines.\n- Unscored regions are rendered as complement intervals over full document length.\n- Chunk boundaries may shift after edits because overlapping chunk descriptors are replaced by the newest analysis.\n\n### Chunking FAQ\n\nQ: If my cursor is near the end of chunk 1 (for example line 999) and I press `Analyze`, what gets analyzed?\n\nA: Binoculars analyzes from the active chunk start, not from the cursor line. In that case it re-analyzes from the start of chunk 1 (often line 1) forward to the current token-limited boundary.\n\nQ: Will chunk boundaries stay fixed forever once discovered?\n\nA: No. Chunk metrics are always chunk-local, but chunk boundaries are operational. If text edits change token density, a later re-analysis can move the chunk end earlier or later, and overlapping prior chunk descriptors are replaced by the new result.\n\n## Safety / Responsible Use\n\nTreat outputs as probabilistic signals within a broader review process. Do not use this tool as the sole basis for punitive or high-stakes actions.\n\n## Appendix: GPTZero vs Binoculars (Public Information)\n\nThis appendix summarizes public details about GPTZero and compares them to the Binoculars method implemented here.\n\n### 1) What Is Known About Gptzero\n\nPublic GPTZero materials describe a layered detection system.\n\n- The initial release (January 2023) focused on `perplexity` and `burstiness` as core indicators.\n- Current documentation describes a probabilistic, sentence-level and document-level deep-learning detector that does not rely solely on perplexity or burstiness.\n- GPTZero states its production system combines multiple components and outputs trinary sentence labels (`human`, `mixed`, `AI`) with confidence and uncertainty handling.\n\nDetails not publicly disclosed:\n\n- Exact model architectures.\n- Training data composition.\n- Post-processing and thresholding methods.\n- Adversarial hardening techniques.\n\nTherefore, GPTZero is partially documented, but the full internal mechanisms remain proprietary.\n\n### 2) How Binoculars Differs\n\nBinoculars is more explicit in its mechanism.\n\n- It is defined by transparent equations over two related language models:\n  - observer `logPPL`\n  - observer-vs-performer `logXPPL`\n  - ratio `B = logPPL / logXPPL`\n- It uses a zero-shot detection approach, requiring no target-model-specific training for the detector.\n- The paper reports strong low-FPR performance, detecting over 90% of generated samples at 0.01% FPR in tested scenarios.\n- Head-to-head comparisons in the paper show Binoculars outperforming GPTZero in their 2023 evaluation setup.\n\n### 3) Why Binoculars Can Compete with Gptzero\n\nAvailable evidence indicates Binoculars can operate in the same competitive tier, with some caveats.\n\n- Published results show strong discrimination at very low false-positive rates, a key deployment factor.\n- The mechanism is model-agnostic in the zero-shot sense and can generalize to unseen generators when assumptions hold.\n- The approach is inspectable and reproducible (equations and open implementation), aiding calibration and operational trust.\n\nHowever, caution is warranted.\n\n- “As robust as GPTZero” is context-dependent and should be validated on your domain, document lengths, and attack or perturbation conditions.\n- The Binoculars paper notes important limits, such as degraded recall in some low-resource language settings and no guarantee against motivated adversarial evasion.\n- Independent benchmarks indicate that many detectors degrade under perturbation, so robustness claims should be treated as empirical and subject to ongoing review.\n\nIn practice:\n\n- Binoculars is credibly “same-league” with commercial detectors in several reported scenarios, especially when low-FPR behaviour is required.\n- You should run periodic, domain-specific benchmark checks (including perturbed or paraphrased text) before making strong operational claims.\n\n### 4) Sources\n\n- Binoculars paper (arXiv): https://arxiv.org/abs/2401.12070\n- Binoculars paper in repo: `background/2401.12070v3.pdf`\n- GPTZero technology page: https://gptzero.me/technology\n- GPTZero FAQ (method overview): https://gptzero.me/faqs/how-does-ai-detection-work\n- GPTZero original launch note (perplexity/burstiness framing): https://gptzero.me/news/first-release-of-gptzero-for-educators-january-3-2023\n- RAID benchmark (robustness context): https://arxiv.org/abs/2406.07958\n",
    "has_analysis": false,
    "b_score_stale": false,
    "last_b_score": null,
    "last_analysis_status_core": "",
    "last_analysis_metrics": null,
    "analyzed_char_end": 0,
    "analysis_chunks": [],
    "analysis_chunk_id_seq": 0,
    "analysis_covered_until": 0,
    "analysis_next_available": false,
    "edited_ranges": [],
    "rewrite_ranges": [],
    "prior_low_ranges": [],
    "prior_high_ranges": []
  }
}